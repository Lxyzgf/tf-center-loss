{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from random import shuffle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchLoader(object):\n",
    "\n",
    "    def __init__(self, file_path, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.labels, self.im_list = self.image_dir_processor(file_path)\n",
    "        self.idx = 0\n",
    "        self.data_num = len(self.labels)\n",
    "        self.rnd_list = np.arange(self.data_num)\n",
    "        shuffle(self.rnd_list)\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i in xrange (self.batch_size):\n",
    "            if self.idx != self.data_num:\n",
    "                cur_idx = self.rnd_list[self.idx]\n",
    "                im_path = self.im_list[cur_idx]\n",
    "                image = cv2.imread(im_path)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(self.labels[cur_idx])\n",
    "\n",
    "                self.idx +=1\n",
    "            else:\n",
    "                self.idx = 0\n",
    "                shuffle(self.rnd_list)\n",
    "\n",
    "        batch_images = np.array(batch_images).astype(np.float32)\n",
    "        batch_labels = np.array(batch_labels).astype(np.float32)\n",
    "        #print(\"the img shape\",np.shape(batch_images))\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def image_dir_processor(self, file_path):\n",
    "        labels = []\n",
    "        im_path_list = []\n",
    "        if not os.path.exists(file_path):\n",
    "            print (\"File %s not exists.\" % file_path)\n",
    "            exit()\n",
    "\n",
    "        with open(file_path, \"r\") as fr:\n",
    "            for line in fr.readlines():\n",
    "                terms = line.rstrip().split()\n",
    "                label = int(terms[1])\n",
    "                im_path_list.append(terms[0])\n",
    "                labels.append(label)\n",
    "\n",
    "        return labels, im_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "LAMBDA = 0.0\n",
    "CENTER_LOSS_ALPHA = 0.0\n",
    "NUM_CLASSES = 526\n",
    "checkpoint_dir = \"./model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    input_images = tf.placeholder(tf.float32, shape=(None,300,300,3), name='input_images')\n",
    "    labels = tf.placeholder(tf.int64, shape=(None), name='labels')\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_center_loss(features, labels, alpha, num_classes):\n",
    "    \"\"\"获取center loss及center的更新op\n",
    "\n",
    "    Arguments:\n",
    "        features: Tensor,表征样本特征,一般使用某个fc层的输出,shape应该为[batch_size, feature_length].\n",
    "        labels: Tensor,表征样本label,非one-hot编码,shape应为[batch_size].\n",
    "        alpha: 0-1之间的数字,控制样本类别中心的学习率,细节参考原文.\n",
    "        num_classes: 整数,表明总共有多少个类别,网络分类输出有多少个神经元这里就取多少.\n",
    "\n",
    "    Return：\n",
    "        loss: Tensor,可与softmax loss相加作为总的loss进行优化.\n",
    "        centers: Tensor,存储样本中心值的Tensor，仅查看样本中心存储的具体数值时有用.\n",
    "        centers_update_op: op,用于更新样本中心的op，在训练时需要同时运行该op，否则样本中心不会更新\n",
    "    \"\"\"\n",
    "    # 获取特征的维数，例如256维\n",
    "    # print features.get_shape()\n",
    "    len_features = features.get_shape()[1]\n",
    "    # 建立一个Variable,shape为[num_classes, len_features]，用于存储整个网络的样本中心，\n",
    "    # 设置trainable=False是因为样本中心不是由梯度进行更新的\n",
    "    centers = tf.get_variable('centers', [num_classes, len_features], dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "    # 将label展开为一维的，输入如果已经是一维的，则该动作其实无必要\n",
    "    labels = tf.reshape(labels, [-1])\n",
    "\n",
    "    # 根据样本label,获取mini-batch中每一个样本对应的中心值\n",
    "    centers_batch = tf.gather(centers, labels)\n",
    "    # 计算loss\n",
    "    loss = tf.nn.l2_loss(features - centers_batch)\n",
    "\n",
    "    # 当前mini-batch的特征值与它们对应的中心值之间的差\n",
    "    diff = centers_batch - features\n",
    "\n",
    "    # 获取mini-batch中同一类别样本出现的次数,了解原理请参考原文公式(4)\n",
    "    unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
    "    appear_times = tf.gather(unique_count, unique_idx)\n",
    "    appear_times = tf.reshape(appear_times, [-1, 1])\n",
    "\n",
    "    diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
    "    diff = alpha * diff\n",
    "\n",
    "    centers_update_op = tf.scatter_sub(centers, labels, diff)\n",
    "\n",
    "    return loss, centers, centers_update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_images):\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "                         activation_fn=tflearn.prelu, stride=1, padding='SAME',\n",
    "                         weights_initializer=tf.truncated_normal_initializer(stddev=0.01)):\n",
    "                         # weights_initializer=tf.contrib.layers.xavier_initializer()):\n",
    "        x = slim.conv2d(input_images, 32, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv1a')\n",
    "\n",
    "        x = slim.conv2d(x, 64, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv1b')\n",
    "\n",
    "        pool1b = slim.max_pool2d(x, [2, 2], stride=2, padding='VALID', scope='pool1b')\n",
    "\n",
    "        conv2_1 = slim.conv2d(pool1b, 64, [3, 3], scope='conv2_1')\n",
    "        conv2_2 = slim.conv2d(conv2_1, 64, [3, 3], scope='conv2_2')\n",
    "        res2_2 = pool1b + conv2_2\n",
    "        conv2 = slim.conv2d(res2_2, 128, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv2')\n",
    "\n",
    "        pool2 = slim.max_pool2d(conv2, [2, 2], stride=2, padding='VALID', scope='pool2')\n",
    "        conv3_1 = slim.conv2d(pool2, 128, [3, 3], scope='conv3_1')\n",
    "        conv3_2 = slim.conv2d(conv3_1, 128, [3, 3], scope='conv3_2')\n",
    "        res3_2 = pool2 + conv3_2\n",
    "\n",
    "        conv3_3 = slim.conv2d(res3_2, 128, [3, 3], scope='conv3_3')\n",
    "        conv3_4 = slim.conv2d(conv3_3, 128, [3, 3], scope='conv3_4')\n",
    "        res3_4 = res3_2 + conv3_4\n",
    "\n",
    "        conv3 = slim.conv2d(res3_4, 256, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv3')\n",
    "        pool3 = slim.max_pool2d(conv3, [2, 2], stride=2, padding='VALID', scope='pool3')\n",
    "        conv4_1 = slim.conv2d(pool3, 256, [3, 3], scope='conv4_1')\n",
    "        conv4_2 = slim.conv2d(conv4_1, 256, [3, 3], scope='conv4_2')\n",
    "        res4_2 = pool3 + conv4_2\n",
    "\n",
    "        conv4_3 = slim.conv2d(res4_2, 256, [3, 3], scope='conv4_3')\n",
    "        conv4_4 = slim.conv2d(conv4_3, 256, [3, 3], scope='conv4_4')\n",
    "        res4_4 = res4_2 + conv4_4\n",
    "\n",
    "        conv4_5 = slim.conv2d(res4_4, 256, [3, 3], scope='conv4_5')\n",
    "        conv4_6 = slim.conv2d(conv4_5, 256, [3, 3], scope='conv4_6')\n",
    "        res4_6 = res4_4 + conv4_6\n",
    "\n",
    "        conv4_7 = slim.conv2d(res4_6, 256, [3, 3], scope='conv4_7')\n",
    "        conv4_8 = slim.conv2d(conv4_7, 256, [3, 3], scope='conv4_8')\n",
    "        res4_8 = res4_6 + conv4_8\n",
    "\n",
    "        conv4_9 = slim.conv2d(res4_8, 256, [3, 3], scope='conv4_9')\n",
    "        conv4_10 = slim.conv2d(conv4_9, 256, [3, 3], scope='conv4_10')\n",
    "        res4_10 = res4_8 + conv4_10\n",
    "\n",
    "        conv4 = slim.conv2d(res4_10, 512, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv4')\n",
    "        pool4 = slim.max_pool2d(conv4, [2, 2], stride=2, padding='VALID', scope='pool4')\n",
    "\n",
    "        conv5_1 = slim.conv2d(pool4, 512, [3, 3], scope='conv5_1')\n",
    "        conv5_2 = slim.conv2d(conv5_1, 512, [3, 3], scope='conv5_2')\n",
    "        res5_2 = pool4 + conv5_2\n",
    "\n",
    "        conv5_3 = slim.conv2d(res5_2, 512, [3, 3], scope='conv5_3')\n",
    "        conv5_4 = slim.conv2d(conv5_3, 512, [3, 3], scope='conv5_4')\n",
    "        res5_4 = res5_2 + conv5_4\n",
    "\n",
    "        conv5_5 = slim.conv2d(res5_4, 512, [3, 3], scope='conv5_5')\n",
    "        conv5_6 = slim.conv2d(conv5_5, 512, [3, 3], scope='conv5_6')\n",
    "        res5_6 = res5_4 + conv5_6\n",
    "        pool5 = slim.max_pool2d(res5_6, [2, 2], stride=2, padding='VALID', scope='pool5')\n",
    "        flatten = slim.flatten(pool5, scope='flatten')\n",
    "        feature = slim.fully_connected(flatten, num_outputs=512, activation_fn=None,\n",
    "                            weights_initializer=tf.contrib.layers.xavier_initializer(), scope='fc1')\n",
    "\n",
    "        x = slim.fully_connected(feature, num_outputs=NUM_CLASSES, activation_fn=None, scope='fc2')\n",
    "\n",
    "    return x, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(input_images, labels, ratio=0.5):\n",
    "    logits, features = inference(input_images)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        with tf.name_scope('center_loss'):\n",
    "            center_loss, centers, centers_update_op = get_center_loss(features, labels, CENTER_LOSS_ALPHA, NUM_CLASSES)\n",
    "        with tf.name_scope('softmax_loss'):\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "        with tf.name_scope('total_loss'):\n",
    "            total_loss = softmax_loss + ratio * center_loss\n",
    "\n",
    "    with tf.name_scope('acc'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(logits, 1), labels), tf.float32))\n",
    "\n",
    "    with tf.name_scope('loss/'):\n",
    "        tf.summary.scalar('CenterLoss', center_loss)\n",
    "        tf.summary.scalar('SoftmaxLoss', softmax_loss)\n",
    "        tf.summary.scalar('TotalLoss', total_loss)\n",
    "\n",
    "    return logits, features, total_loss, accuracy, centers_update_op, center_loss, softmax_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits, features, total_loss, accuracy, centers_update_op, center_loss, softmax_loss = build_network(input_images, labels, ratio=LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch_loader = BatchLoader(\"./data/facescrub_train.list\", 128)\n",
    "test_batch_loader = BatchLoader(\"./data/facescrub_val.list\", 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "with tf.control_dependencies([centers_update_op]):\n",
    "    train_op = optimizer.minimize(total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()\n",
    "#sess = tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#writer = tf.summary.FileWriter('/tmp/face_log', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('/tmp/face_log', sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    step = sess.run(global_step)\n",
    "    while step <= 80000:\n",
    "        batch_images, batch_labels = train_batch_loader.next_batch()\n",
    "        # print batch_images.shape\n",
    "        # print batch_labels.shape\n",
    "        _, summary_str, train_acc, Center_loss, Softmax_loss = sess.run(\n",
    "            [train_op, summary_op, accuracy, center_loss, softmax_loss],\n",
    "            feed_dict={\n",
    "                input_images: (batch_images - 127.5) * 0.0078125, \n",
    "                labels: batch_labels,\n",
    "            })\n",
    "        step += 1\n",
    "        if step % 1 == 0:\n",
    "            print (\"********* Step %s: ***********\" % str(step))\n",
    "            print (\"center loss: %s\" % str(Center_loss))\n",
    "            print (\"softmax_loss: %s\" % str(Softmax_loss))\n",
    "            print (\"train_acc: %s\" % str(train_acc))\n",
    "            print (\"*******************************\")\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            saver.save(sess, checkpoint_dir + 'model.ckpt', global_step=step)\n",
    "\n",
    "        writer.add_summary(summary_str, global_step=step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            batch_images, batch_labels = test_batch_loader.next_batch()\n",
    "            vali_image = (batch_images - 127.5) * 0.0078125\n",
    "            vali_acc = sess.run(\n",
    "                accuracy,\n",
    "                feed_dict={\n",
    "                    input_images: vali_image,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "            print((\"step: {}, train_acc:{:.4f}, vali_acc:{:.4f}\".\n",
    "                  format(step, train_acc, vali_acc)))\n",
    "\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
